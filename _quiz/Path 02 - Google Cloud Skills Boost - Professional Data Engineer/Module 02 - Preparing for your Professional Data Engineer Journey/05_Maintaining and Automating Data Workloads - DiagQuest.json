{
  "quiz": {
    "title": "Google Cloud Platform Advanced Operations Quiz",
    "score": 80,
    "passingScore": 75,
    "congratulationsMessage": "Congratulations! You passed this assessment.",
    "questions": [
      {
        "questionText": "When running Dataflow jobs, you see this error in the logs: 'A hot key HOT_KEY_NAME was detected inâ€¦'. You need to resolve this issue and make the workload performant. What should you do?",
        "options": [
          "A. Add more compute instances for processing.",
          "B. Disable Dataflow shuffle.",
          "C. Increase the data with the hot key.",
          "D. Ensure that your data is evenly distributed."
        ],
        "answerIndex": [3],
        "explanation": "The Dataflow transformations are more performant with an evenly distributed key."
      },
      {
        "questionText": "You have a team of data analysts that run queries interactively on BigQuery during work hours. You also have thousands of report generation queries that run simultaneously. You often see an error: Exceeded rate limits: too many concurrent queries for this project_and_region. How would you resolve this issue?",
        "options": [
          "A. Create a yearly reservation of BigQuery slots.",
          "B. Create a view to run the queries.",
          "C. Run all queries in interactive mode.",
          "D. Run the report generation queries in batch mode."
        ],
        "answerIndex": [3],
        "explanation": "Offloading the report generation queries to batch mode reduces the number of concurrent queries."
      },
      {
        "questionText": "You have a Dataflow pipeline in production. For certain data, the system seems to be stuck longer than usual. This is causing delays in the pipeline execution. You want to reliably and proactively track and resolve such issues. What should you do?",
        "options": [
          "A. Set up alerts with Cloud Functions code that reviews the audit logs regularly.",
          "B. Review the Cloud Monitoring dashboard regularly.",
          "C. Review the Dataflow logs regularly.",
          "D. Set up alerts on Cloud Monitoring based on system lag."
        ],
        "answerIndex": [3],
        "explanation": "Setting up alerts proactively notifies users about issues or metrics that need to be tracked."
      },
      {
        "questionText": "You are running a Dataflow pipeline in production. The input data for this pipeline is occasionally inconsistent. Separately from processing the valid data, you want to efficiently capture the erroneous input data for analysis. What should you do?",
        "options": [
          "A. Read the data once, and split it into two pipelines, one to output valid data and another to output erroneous data.",
          "B. Re-read the input data and create separate outputs for valid and erroneous data.",
          "C. Create a side output for the erroneous data.",
          "D. Check for the erroneous data in the logs."
        ],
        "answerIndex": [2],
        "explanation": "Using side outputs can collect the erroneous data efficiently and is a recommended approach."
      },
      {
        "questionText": "You need to create repeatable data processing tasks by using Cloud Composer. You need to follow best practices and recommended approaches. What should you do?",
        "options": [
          "A. Write each task to be responsible for one operation.",
          "B. Use current time with the now() function for computation.",
          "C. Update data with INSERT statements during the task run.",
          "D. Combine multiple functionalities in a single task execution."
        ],
        "answerIndex": [0],
        "explanation": "To run repeatable tasks, it is recommended to use atomic tasks that have a single responsibility. Many of these tasks can be combined in sequence to achieve a desired end result."
      },
      {
        "questionText": "A colleague at Cymbal Retail asks you about the configuration of Dataproc autoscaling for a project. What would be the Google-recommended situation when you should enable autoscaling?",
        "options": [
          "A. When you want to scale on-cluster Hadoop Distributed File System (HDFS).",
          "B. When you want to down-scale idle clusters to minimum size.",
          "C. When you want to scale out single-job clusters.",
          "D. When there are different size workloads on the cluster."
        ],
        "answerIndex": [2],
        "explanation": "Single job clusters are well suited for autoscaling because there won't be any overlap with scaling of other jobs."
      },
      {
        "questionText": "You need to design a Dataproc cluster to run multiple small jobs. Many jobs (but not all) are of high priority. What should you do?",
        "options": [
          "A. Reuse the same cluster to run all jobs in parallel.",
          "B. Use ephemeral clusters.",
          "C. Reuse the same cluster and run each job in sequence.",
          "D. Use cluster autoscaling."
        ],
        "answerIndex": [1],
        "explanation": "Using ephemeral clusters allows you to spin up clusters as needed for particular jobs, ensuring resources are optimally used and prioritized."
      },
      {
        "questionText": "You run a Cloud SQL instance for a business that requires that the database is accessible for transactions. You need to ensure minimal downtime for database transactions. What should you do?",
        "options": [
          "A. Configure replication.",
          "B. Configure backups and increase the number of backups.",
          "C. Configure high availability.",
          "D. Configure backups."
        ],
        "answerIndex": [2],
        "explanation": "Configuring high availability on Cloud SQL will automatically switch to the secondary instance when the primary instance goes down, thus reducing downtime for the database's users."
      },
      {
        "questionText": "Cymbal Retail processes streaming data on Dataflow with Pub/Sub as a source. You need to plan for disaster recovery and protect against zonal failures. What should you do?",
        "options": [
          "A. Create Dataflow jobs from templates.",
          "B. Take Dataflow snapshots periodically.",
          "C. Enable Dataflow shuffle.",
          "D. Enable vertical autoscaling."
        ],
        "answerIndex": [0],
        "explanation": "Creating jobs from templates makes it easy to start new jobs by changing parameter values. However, since templates do not retain any state, they are not useful for disaster recovery."
      },
      {
        "questionText": "Multiple analysts need to prepare reports on Monday mornings due to which there is heavy utilization of BigQuery. You want to take a cost-effective approach to managing this demand. What should you do?",
        "options": [
          "A. Use on-demand pricing.",
          "B. Use BigQuery Enterprise Plus edition with a three-year commitment.",
          "C. Use BigQuery Enterprise edition with a one-year commitment.",
          "D. Use Flex Slots."
        ],
        "answerIndex": [3],
        "explanation": "Flex Slots let you reserve BigQuery slots for short durations."
      }
    ]
  }
}
